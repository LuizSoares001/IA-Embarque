# -*- coding: utf-8 -*-
"""TP_IA_EMBARQUEE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iKHeXPCFbghnl78NwaLu2nLqccQaNjBJ

## **PRACTICAL SESSION 1** — Deep Learning for predictive maintenance

The dataset used is the **AI4I 2020** Predictive Maintenance Dataset, which contains 10,000 instances of industrial sensor data. Each instance represents the operating condition of a machine and is associated with a label indicating whether a failure has occurred and, if so, what type of failure it is.

The 5 possible labels are:



*   **TWF**: Tool Wear Failure
*   **HDF**: Heat Dissipation Failure
*   **PWF**: Power Failure
*   **OSF**: Overstrain Failure
*   **RNF**: Random Failure


The data is available on eCAMPUS as CSV file called: "ai4i2020.csv"

## **PRACTICAL SESSION Goal** — Ceate a deep leanring model allowing to realize a predictive maintenance mission

## **1 - Analysis of the dataset**

All libraries used ***SHOULD BE PLACED*** in the code cell below
"""

<<<<<<< Updated upstream
!pip install tensorflow==2.12.0

=======
>>>>>>> Stashed changes
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#TensorFlow e Keras
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2
from tensorflow.keras.metrics import AUC
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

#Scikit-Learn
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.metrics import (
    confusion_matrix, classification_report, roc_curve, auc,
    ConfusionMatrixDisplay, multilabel_confusion_matrix
)
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

#Imbalanced-Learn
from imblearn.over_sampling import SMOTE, SMOTENC
from imblearn.under_sampling import RandomUnderSampler

"""**QUESTION:** Load dataset and display some lines of the csv file."""

data_path = "/content/ai4i2020.csv"

<<<<<<< Updated upstream
df = pd.read_csv(data_path)

=======

df = pd.read_csv(data_path)


>>>>>>> Stashed changes
df.head()

"""**QUESTION:** Display the distribution of machine failures and non-failures with a bar graph."""

counts_machine_failure = df['Machine failure'].value_counts()

<<<<<<< Updated upstream
labels = ['No Failure', 'Failure']
values = [counts_machine_failure.get(0, 0), counts_machine_failure.get(1, 0)]

=======

labels = ['No Failure', 'Failure']
values = [counts_machine_failure.get(0, 0), counts_machine_failure.get(1, 0)]


>>>>>>> Stashed changes
plt.figure(figsize=(6, 4))
plt.bar(labels, values, color=['blue', 'red'])
plt.title("Distribution of Machine Failures vs Non-Failures")
plt.xlabel("Failure Status")
plt.ylabel("Number of Machines")

<<<<<<< Updated upstream
for i, v in enumerate(values):
    plt.text(i, v + 0.5, str(int(v)), ha='center')
=======

for i, v in enumerate(values):
    plt.text(i, v + 0.5, str(int(v)), ha='center')

>>>>>>> Stashed changes
plt.show()

"""**ANALYSIS QUESTION:** What do you observe?

<<<<<<< Updated upstream
There are many more machines without failures compared to machines that experienced failures.

**ANALYSIS QUESTION:** What will be the consequence of this phenomenon on the model's learning?

Subsequently, training the AI on such imbalanced data may result in a model that performs well in identifying non-failure cases, but poorly in detecting actual failures.
=======
Há muitas maquinas que não houveram falhas em compararação com máquinas que tiveram falhas.

**ANALYSIS QUESTION:** What will be the consequence of this phenomenon on the model's learning?

Porteriormente ao usar esses dados para treinar a IA, pode acarretar em um bom aprendizado para detectar apenas quando não há falhas, resultando em mal aprendizado para identificar quando há falhas também.
>>>>>>> Stashed changes

**QUESTION:** Create a bar chart showing the distribution of different failure types (TWF, HDF, PWF, OSF, RNF). Display the exact values above each bar in the chart."
"""

failure_types = ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']
failure_counts = df[failure_types].sum()

plt.figure(figsize=(8, 6))
ax = sns.barplot(x=failure_counts.index, y=failure_counts.values, palette=['blue', 'orange', 'green', 'red', 'purple'] )

for i, value in enumerate(failure_counts.values):
    ax.text(i, value + 2, str(value), ha='center', fontsize=12)

plt.xlabel("Type of fault")
plt.ylabel("Number of Occurrences")
plt.title("Distribution of Fault Types")

plt.show()

"""**ANALYSIS QUESTION:** What do you observe?

<<<<<<< Updated upstream
Even when comparing only the machines that experienced failures, there is still a noticeable imbalance among the types of failures: 19 were categorized as RNF and 115 as HDF. However, RNF will be removed from the dataset since it represents random failures, leaving TWF as a failure type with a significant discrepancy compared to the others.
=======
Mesmo em comparação com as maquinas que tiveram falhas, ainda sim há um diferença entre os tipos de falhas, onde 19 foram categorizadas como RNF e 115 foram caracterizadas como HDF.
>>>>>>> Stashed changes

**QUESTION:** Create a bar chart showing the distribution of failure types (TWF, HDF, PWF, OSF, RNF) among machines that experienced a failure (Machine failure == 1). Additionally, add a "No Specific Failure" category to count cases where a machine failed but no specific failure type was recorded. Display the exact values above each bar in the chart."
"""

failed_machines = df[df["Machine failure"] == 1]

failure_types = ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']
failure_counts = failed_machines[failure_types].sum()

no_specific_failure = df[(df["Machine failure"] == 0) & (df["RNF"] == 1)].shape[0]

failure_counts["No Specific Failure"] = no_specific_failure

plt.figure(figsize=(8, 6))
ax = sns.barplot(x=failure_counts.index, y=failure_counts.values,  palette=['blue', 'orange', 'green', 'red', 'purple', 'gray'])

for i, value in enumerate(failure_counts.values):
    ax.text(i, value + 2, str(value), ha='center', fontsize=12)

plt.xlabel("Type of fault")
plt.ylabel("Number of Occurrences")
plt.title("Distribution of Fault Types among Faulty Machines")

plt.show()

"""**ANALYSIS QUESTION:** What do you obsrve comapred to the previous question ? What can you conclude?

<<<<<<< Updated upstream
Once again, there is a discrepancy in the number of samples among the machines that experienced failures. Within the RNF category, most of them were actually No Specific Failures.
=======
Mais uma vez há uma discrepancia no número de amostras nas maquinás que obtiveram falhas, onde apenas 1 foi caracterizada como RNF e 115 como HDF
>>>>>>> Stashed changes

**QUESTION:** Display the names of the different columns in the dataset with their respective data types.
"""

print("Column Names and Data Types:")
print(df.dtypes)

"""**ANALYSIS QUESTION:** To train the model, what will be the inputs and outputs (What are the names of the columns that you will use?)? Justify your response.
<<<<<<< Updated upstream
Remember, you want to predict if the machine will fail, and if so, what kind of failure. You need to use previous results to justify your response.

To train the model, the chosen input variables will be those that indicate the machine's usage and physical conditions, such as Air temperature, Process temperature, Rotational speed, Torque, Tool wear, and Type, as they directly influence performance and the occurrence of failures.
The output variables will consist of the No failure column (created to represent cases where no failure occurred) and the columns of specific failure types such as TWF, HDF, PWF, and OSF.
The RNF column was removed, because in addition to representing random failures, its amount is negligible compared to the others.
Since the goal is to indicate the type of failure that occurred, the Machine failure column will be replaced by No failure, as this becomes more viable, given that the specific failure columns already indicate whether a failure occurred and what type it was, avoiding redundancies in the model.
=======
Remember, you want to predict if the machine will fail, and if so, what kind of failure. You need to yse previous results to jsurtify your response.

Para treinar o modelo, as entradas escolhidas serão as variáveis que indicam o uso da máquina e situação fisica dela, como Air temperature, Process temperature, Rotational speed, Torque, Tool wear e Type, pois influenciam diretamente no desempenho e possíveis falhas. As saídas
serão Machine failure e as colunas com os tipos de falhas como TWF, HDF, PWF e OSF, retirei RNF pois sua quantidade é irrisória em comparação com as outras falhas e principalmente porque são falhas aleatórias. Isso acarreta em uma previsão sobre os possiveis problemas além de permitir ações de manutenção preditiva baseadas nas condições da máquina.
>>>>>>> Stashed changes

## **2- Train model Without balancing the dataset**

---

In this section, you must build and train a model without rebalancing the dataset.

**QUESTION:** Create X_train, Y_train, X_test, and Y_test. How many elements are present in X_train, Y_train, X_test, and Y_test? (Print the values)
"""

<<<<<<< Updated upstream
# Create dummy variables for the 'Type' column
df = pd.get_dummies(df, columns=['Type'], drop_first=False)

#  Failure types
failure_types = ['TWF', 'HDF', 'PWF', 'OSF']

#  Remove samples with more than one specific failure
df['sum_failures'] = df[failure_types].sum(axis=1)
df = df[(df['Machine failure'] == 0) | ((df['Machine failure'] == 1) & (df['sum_failures'] == 1))]
df.drop(columns='sum_failures', inplace=True)

#  Create "No failure" column
df['No failure'] = (df['Machine failure'] == 0).astype(int)

#  Copy columns as Y base
Y = df[['No failure', 'TWF', 'HDF', 'PWF', 'OSF']].copy()

Y = Y.apply(lambda row: (row == 1).astype(int), axis=1)

#  Define X (remove output and irrelevant columns)
df = df.drop(columns=['class'], errors='ignore')
X = df.drop(columns=['UDI', 'Product ID', 'Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF', 'No failure'])

#  Normalize X
=======
# Criar variáveis dummies para a coluna 'Type'
df = pd.get_dummies(df, columns=['Type'], drop_first=False)

# Definir X (remover IDs e variáveis de saída)
X = df.drop(columns=['UDI', 'Product ID', 'Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'])

# Definir y (agora multi-label: cada falha é independente)
Y = df[['Machine failure','TWF', 'HDF', 'PWF', 'OSF']]

# Criar coluna "No Failure"
#Y['No Failure'] = (df['Machine failure'] == 0).astype(int)

# Normalizar X para intervalo [0,1]
>>>>>>> Stashed changes
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

<<<<<<< Updated upstream
#  Train/test split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y.values.argmax(axis=1))

#  Show dimensions
print(f"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}")
print(f"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}")

print("\nX:")

print(X.head())

print("\nY:")

print(Y.head())

# Specific failure types
failure_types = ['TWF', 'HDF', 'PWF', 'OSF']

# Create column with the sum of failures per row
df['sum_failures'] = df[failure_types].sum(axis=1)

# Filter only machines with failure AND a single specific failure
filtered_failures = df[(df['Machine failure'] == 1) & (df['sum_failures'] == 1)]

# Count how many unique samples remain
num_remaining = filtered_failures.shape[0]
print(f"Number of samples with only one specific failure: {num_remaining}")

# Count how many of each failure type exist in the filtered samples
remaining_failure_counts = filtered_failures[failure_types].sum()

# Plot the distribution of failure types
plt.figure(figsize=(8, 6))
ax = sns.barplot(x=remaining_failure_counts.index, y=remaining_failure_counts.values,
                 palette=['blue', 'orange', 'green', 'red'])

# Add values above the bars
for i, value in enumerate(remaining_failure_counts.values):
    ax.text(i, value + 1, str(int(value)), ha='center', fontsize=12)

plt.xlabel("Failure Type")
plt.ylabel("Number of Occurrences")
plt.title("Distribution of Unique Failures After Filtering")
plt.show()

# Remove the auxiliary column
df.drop(columns='sum_failures', inplace=True)

=======
# print das colunas de x e y
print(X.head())
print(Y.head())
# Divisão treino (70%) e teste (30%)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Exibir dimensões
print(f"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}")
print(f"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}")

>>>>>>> Stashed changes
"""



**QUESTION** Code below the model architecture"""

<<<<<<< Updated upstream
# Model for multi-class classification (5 exclusive outputs)
=======
# Criar o modelo aprimorado para multi-label
>>>>>>> Stashed changes
model = Sequential([
    Dense(64, kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),
    BatchNormalization(),
    LeakyReLU(),
<<<<<<< Updated upstream
    Dropout(0.2),

    Dense(32, kernel_regularizer=l2(0.01)),
    #BatchNormalization(),
    LeakyReLU(),
    Dropout(0.2),

    Dense(5, activation='softmax')  #  using softmax for exclusive multi-class
])

# Compilation with categorical_crossentropy
model.compile(
    optimizer=AdamW(learning_rate=0.001, weight_decay=0.02),
    loss='categorical_crossentropy',           # one-hot multi-class
    metrics=['accuracy']
)

# Display the summary
=======
    Dropout(0.3),

    Dense(32, kernel_regularizer=l2(0.005)),
    BatchNormalization(),
    LeakyReLU(),
    Dropout(0.2),

    Dense(5, activation='sigmoid')  # 5 classes: Machine Failure, TWF, HDF, PWF, OSF
])

# Compilar o modelo com métricas ajustadas
model.compile(optimizer=AdamW(learning_rate=0.001, weight_decay=0.02),
              loss='binary_crossentropy',  # Multi-label classification
              metrics=['binary_accuracy', AUC(name='auc')])

# Exibir resumo
>>>>>>> Stashed changes
model.summary()

"""**QUESTION** Code below the algorithms allowing to train model

**WARNING!** You need to plot the training and test accuracy and loss to check if our model is overfitting
"""

<<<<<<< Updated upstream
#  Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)

#  Train the model
=======
# Definir callbacks otimizados
early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)

# Treinar modelo
>>>>>>> Stashed changes
history = model.fit(
    X_train, Y_train,
    validation_data=(X_test, Y_test),
    epochs=50,
    batch_size=64,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

<<<<<<< Updated upstream
#  Function to plot metrics
def plot_metric(history, metric, title, ylabel, loc='upper left'):
    if metric in history.history:
        plt.plot(history.history[metric], label='Train')
    if f'val_{metric}' in history.history:
        plt.plot(history.history[f'val_{metric}'], label='Validation')

    plt.title(title)
    plt.xlabel('Epoch')
=======
#Função para plotar métricas corretamente para multi-label
def plot_metric(history, metric, title, ylabel, loc='upper left'):
    if metric in history.history:
        plt.plot(history.history[metric], label='Treino')
    if f'val_{metric}' in history.history:
        plt.plot(history.history[f'val_{metric}'], label='Teste')

    plt.title(title)
    plt.xlabel('Época')
>>>>>>> Stashed changes
    plt.ylabel(ylabel)
    plt.legend(loc=loc)
    plt.show()

<<<<<<< Updated upstream
#  Plot the curves
plot_metric(history, 'loss', 'Model Loss', 'Loss', loc='upper right')
plot_metric(history, 'accuracy', 'Model Accuracy', 'Accuracy')
=======
#Plotar a perda (loss)
plot_metric(history, 'loss', 'Perda (Loss) do Modelo', 'Loss', loc='upper right')

#  Plotar acurácia binária (para multi-label)
plot_metric(history, 'binary_accuracy', 'Acurácia Binária do Modelo', 'Acurácia')

#  Plotar AUC (Área Sob a Curva ROC)
plot_metric(history, 'auc', 'Área Sob a Curva ROC (AUC)', 'AUC')
>>>>>>> Stashed changes

"""**QUESTION** Plot the confusion matrix and the classification report

**Tips:**

*   classification report link

> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html

*   Matrix confusion

> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html
"""

<<<<<<< Updated upstream
#  Make predictions (probabilities)
Y_pred_probs = model.predict(X_test)

#  Get the class with the highest probability (softmax) → final prediction
Y_pred = Y_pred_probs.argmax(axis=1)

#  Get the true classes
Y_true = Y_test.values.argmax(axis=1)

#  Class labels
labels = ['No failure', 'TWF', 'HDF', 'PWF', 'OSF']

#  Generate classification report
report = classification_report(Y_true, Y_pred, target_names=labels)
print("\nClassification Report:")
print(report)

#  Create confusion matrix
cm = confusion_matrix(Y_true, Y_pred)

#  Normalize by row (percentage per true class)
cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100

#  Plot 5x5 confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_percent, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=labels, yticklabels=labels)

plt.title("Normalized Confusion Matrix (%)")
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.show()

"""**ANALYSIS QUESTION** What do you observe? What can you conclude?

"The model performs well in predicting when there are no failures, however, it struggles to predict when failures do occur. The predominant prediction tends to be 'no failure', due to the significantly larger number of no failure machines compared to those with failures.
=======
# Fazer previsões em probabilidades
Y_pred_probs = model.predict(X_test)

# Ajustar threshold dinamicamente baseado na média das probabilidades
thresholds = np.mean(Y_pred_probs, axis=0)  # Calcula média por classe
Y_pred = (Y_pred_probs >= thresholds).astype(int)  # Ajusta decisão para cada classe

#  Gerar relatório de classificação
labels = ['Machine Failure', 'TWF', 'HDF', 'PWF', 'OSF']
report = classification_report(Y_test, Y_pred, target_names=labels)
print("\nRelatório de Classificação:")
print(report)

# Criar matrizes de confusão para cada classe
cm_matrices = multilabel_confusion_matrix(Y_test, Y_pred)

# Plotar as matrizes de confusão separadas por classe
for i, label in enumerate(labels):
    cm = cm_matrices[i].astype('float') / cm_matrices[i].sum(axis=1, keepdims=True) * 100  # Normalizar para porcentagem

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues",
                xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
    plt.title(f"Matriz de Confusão - {label} (%)")
    plt.xlabel("Predito")
    plt.ylabel("Real")
    plt.show()

"""**ANALYSIS QUESTION** What do you observe? What can you conclude?

O modelo ficou bom em prever quando não há falhas, porém ele não consegue prever quando há, e sempre a respota que prevalece é que não houve, dado que o numero de maquinas sem falhas é muito maior em comparação com as maquinas com falhas.
>>>>>>> Stashed changes

## **3- Train model With balancing the dataset**

---

Methods for rebalancing a dataset:


*   Use oversampling techniques (e.g., SMOTE) to generate synthetic data for minority classes


> https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html



*   Apply undersampling techniques (e.g., random undersampling, Tomek Links, Edited Nearest Neighbors) to reduce the majority class size



> https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html



*   Use class weighting during model training to penalize errors on minority classes



> https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?hl=fr

**QUESTION:** Create X_train, Y_train, X_test, and Y_test. How many elements are present in X_train, Y_train, X_test, and Y_test? (Print the values)
"""

<<<<<<< Updated upstream
# Undersampling
undersampler = RandomUnderSampler(sampling_strategy=0.3, random_state=42)
X_resampled, y_no_failure = undersampler.fit_resample(X, Y['No failure'])
selected_indices = y_no_failure.index
Y_resampled = Y.iloc[selected_indices].copy()
Y_resampled = Y_resampled.apply(lambda row: (row == 1).astype(int), axis=1)

# Split with stratification
X_train_under, X_test_under, Y_train_under, Y_test_under = train_test_split(
    X_resampled, Y_resampled, test_size=0.2, random_state=42,
    stratify=Y_resampled.values.argmax(axis=1)
)

# Automatically compute class weights
y_integers = Y_train_under.values.argmax(axis=1)
classes = np.unique(y_integers)

class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_integers)
class_weight_dict = {i: weight for i, weight in zip(classes, class_weights)}

print(class_weight_dict)

print(X_train_under.shape, Y_train_under.shape, X_test_under.shape, Y_test_under.shape)

"""**ANALYSIS QUESTION:** Explain the choices you made to balance the dataset.

I used undersampling together with class weight, where undersampling helps to reduce the large difference between the failure variables (TWF, HDF, PWF, OSF) and the machines without failures. Class weight, on the other hand, provides different weights during training, mainly adjusting the imbalance of TWF compared to the others.
I did not use SMOTE because, although it allowed the model to learn well during training with the generated samples, in practice the model tended to correctly predict mostly the synthetic data, showing poor generalization to real data.
=======
data_path = "/content/ai4i2020.csv"
df = pd.read_csv(data_path)

#separa os tipos das maquinas em booleanos
df = pd.get_dummies(df, columns=['Type'], drop_first=False)

#Definir X e Y antes do balanceamento
X = df.drop(columns=['UDI', 'Product ID', 'Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'])
Y = df[['Machine failure', 'TWF', 'HDF', 'PWF', 'OSF']]

# Aplicar Undersampling APENAS para `Machine Failure`
undersampler = RandomUnderSampler(sampling_strategy=0.3, random_state=42)  # Mantém 30% dos "No Failure"
X_resampled, y_machine_failure = undersampler.fit_resample(X, Y['Machine failure'])  # Apenas Machine Failure

# Recuperar os índices selecionados pelo undersampling
selected_indices = y_machine_failure.index

# Filtrar `Y` completo para manter os rótulos corretos das amostras balanceadas
Y_resampled = Y.loc[selected_indices]

# Normalizar X para intervalo [0,1]
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_resampled)
X_resampled = pd.DataFrame(X_scaled, columns=X.columns)

# Agora dividir em treino (70%) e teste (30%)
X_train, X_test, Y_train, Y_test = train_test_split(
    X_resampled, Y_resampled, test_size=0.2, random_state=42)

# Calcular Class Weights automaticamente
class_weights = {}
for i, col in enumerate(Y_train.columns):
    class_weights[i] = compute_class_weight(class_weight="balanced", classes=np.array([0, 1]), y=Y_train[col])

custom_class_weights = {
    0: 1.5,   # Machine Failure (mais importância para melhorar precisão)
    1: 4.0,   # TWF (reduzir para evitar falsos positivos)
    2: 3.0,   # HDF (ajustado)
    3: 3.5,   # PWF (ajustado)
    4: 3.5    # OSF (ajustado)
}


# Exibir os pesos ajustados
print("Pesos calculados automaticamente para class_weight:", class_weights)

# Exibir dimensões finais
print(f"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}")
print(f"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}")

X_train, Y_train
X_test, Y_test
custom_class_weights

# Contar a quantidade de máquinas com e sem falha após o undersampling
failure_counts = Y_resampled['Machine failure'].value_counts()

# 📊 Plotar a distribuição das máquinas após o undersampling
plt.figure(figsize=(6, 4))
ax = sns.barplot(x=failure_counts.index, y=failure_counts.values, palette=['blue', 'red'])

# Adicionar valores acima das barras
for i, value in enumerate(failure_counts.values):
    ax.text(i, value + 2, str(value), ha='center', fontsize=12)

plt.xlabel("Machine Failure (0 = Sem Falha, 1 = Com Falha)")
plt.ylabel("Número de Máquinas")
plt.title("Distribuição de Máquinas com e sem Falha após Undersampling")

# Exibir o gráfico
plt.show()

"""**ANALYSIS QUESTION:** Explain the choices you made to balance the dataset.

Eu fiz a mistura de SMOTE e class weigths, pois com o SMOTE, consigo criar novas amostras das Maquinas com Falhas, enquanto que o classh weigths me permite modificar os pesos das variaveis de treino, assim consigo dar mais peso para as variaveis em menor numero e menor peso para as variaveis abundantes.
>>>>>>> Stashed changes

**QUESTION:** Code below the model architecture

**TIP:** It could be interesting to keep it the same as before
"""

<<<<<<< Updated upstream
# Model for multi-class classification (5 exclusive outputs)
model = Sequential([
    Dense(64, kernel_regularizer=l2(0.01), input_shape=(X_train_under.shape[1],)),
=======
# Criar o modelo aprimorado para multi-label
model = Sequential([
    Dense(64, kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),
    BatchNormalization(),
    LeakyReLU(),
    Dropout(0.3),

    Dense(32, kernel_regularizer=l2(0.01)),
>>>>>>> Stashed changes
    BatchNormalization(),
    LeakyReLU(),
    Dropout(0.2),

<<<<<<< Updated upstream
    Dense(32, kernel_regularizer=l2(0.01)),
    #BatchNormalization(),
    LeakyReLU(),
    Dropout(0.2),

    Dense(5, activation='softmax')  #  using softmax for exclusive multi-class
])

# Compilation with categorical_crossentropy
model.compile(
    optimizer=AdamW(learning_rate=0.001, weight_decay=0.02),
    loss='categorical_crossentropy',           # one-hot multi-class
    metrics=['accuracy']
)

# Display the summary
=======
    Dense(5, activation='sigmoid')  # 5 classes: Machine Failure, TWF, HDF, PWF, OSF
])

# Compilar o modelo com métricas ajustadas
model.compile(optimizer=AdamW(learning_rate=0.001, weight_decay=0.01),
              loss='binary_crossentropy',  # Multi-label classification
              metrics=['binary_accuracy', AUC(name='auc')])

# Exibir resumo
>>>>>>> Stashed changes
model.summary()

"""**QUESTION** Code below the algorithms allowing to train model

"""

<<<<<<< Updated upstream
#  Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)

#  Train the model
history = model.fit(
    X_train_under,
    Y_train_under,
    validation_data=(X_test_under, Y_test_under),
    epochs=50,
    batch_size=64,
    class_weight=class_weight_dict,
=======
# Definir callbacks otimizados
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)

# Criar dicionário de class_weight no formato correto para o modelo Keras
class_weight_dict = {i: class_weights[i][1] for i in range(len(class_weights))}

# Treinar o modelo com os dados balanceados e pesos ajustados
history = model.fit(
    X_train,
    Y_train,
    validation_data=(X_test, Y_test),
    epochs=50,
    batch_size=64,
    #class_weight=class_weight_dict,  # Aplicando class weights automáticos
    #class_weight=custom_class_weights,
>>>>>>> Stashed changes
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

<<<<<<< Updated upstream
#  Function to plot metrics
def plot_metric(history, metric, title, ylabel, loc='upper left'):
    if metric in history.history:
        plt.plot(history.history[metric], label='Train')
    if f'val_{metric}' in history.history:
        plt.plot(history.history[f'val_{metric}'], label='Validation')

    plt.title(title)
    plt.xlabel('Epoch')
=======



#Função para plotar métricas corretamente para multi-label
def plot_metric(history, metric, title, ylabel, loc='upper left'):
    if metric in history.history:
        plt.plot(history.history[metric], label='Treino')
    if f'val_{metric}' in history.history:
        plt.plot(history.history[f'val_{metric}'], label='Teste')

    plt.title(title)
    plt.xlabel('Época')
>>>>>>> Stashed changes
    plt.ylabel(ylabel)
    plt.legend(loc=loc)
    plt.show()

<<<<<<< Updated upstream
#  Plot the curves
plot_metric(history, 'loss', 'Model Loss', 'Loss', loc='upper right')
plot_metric(history, 'accuracy', 'Model Accuracy', 'Accuracy')

#  Save test data and model
np.save('X_test_labels.npy', 'X_test_under')
np.save('Y_test_labels.npy', 'Y_test_under')
model.save('model_test.h5')

"""**QUESTION** Plot the confusion matrix and the classification report"""

#  Make predictions as probabilities
Y_pred_probs = model.predict(X_test_under)

#  Get the class with the highest probability (softmax) → final prediction
Y_pred = Y_pred_probs.argmax(axis=1)

#  Get the true classes
Y_true = Y_test_under.values.argmax(axis=1)

#  Class labels
labels = ['No failure', 'TWF', 'HDF', 'PWF', 'OSF']

#  Generate classification report
report = classification_report(Y_true, Y_pred, target_names=labels)
print("\nClassification Report:")
print(report)

#  Create confusion matrix
cm = confusion_matrix(Y_true, Y_pred)

#  Normalize by row (percentage per true class)
cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100

#  Plot the 5x5 confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_percent, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=labels, yticklabels=labels)

plt.title("Normalized Confusion Matrix (%)")
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.show()

"""**ANALYSIS QUESTION** What do you observe? What can you conclude?

It was greatly improved compared to the data without balancing, since I removed a portion of the samples that had no failures, as their number was too high compared to the machines that had failures.
As previously mentioned, I used class weight to penalize or not certain samples during training, but even so, I was not able to balance the predictions for TWF, as the recall is very high while the precision is low, this indicates that there are many false positives, which I believe is due to the use of class weight.
In any case, the overall model showed an accuracy above 80%, being able to predict the types of failures with greater precision.
=======
#Plotar a perda (loss)
plot_metric(history, 'loss', 'Perda (Loss) do Modelo', 'Loss', loc='upper right')

#  Plotar acurácia binária (para multi-label)
plot_metric(history, 'binary_accuracy', 'Acurácia Binária do Modelo', 'Acurácia')

#  Plotar AUC (Área Sob a Curva ROC)
plot_metric(history, 'auc', 'Área Sob a Curva ROC (AUC)', 'AUC')

"""**QUESTION** Plot the confusion matrix and the classification report"""

# Fazer previsões em probabilidades
Y_pred_probs = model.predict(X_test)

# Ajustar threshold dinamicamente baseado na média das probabilidades
thresholds = np.mean(Y_pred_probs, axis=0)  # Calcula média por classe
Y_pred = (Y_pred_probs >= thresholds).astype(int)  # Ajusta decisão para cada classe

#  Gerar relatório de classificação
labels = ['Machine Failure', 'TWF', 'HDF', 'PWF', 'OSF']
report = classification_report(Y_test, Y_pred, target_names=labels)
print("\nRelatório de Classificação:")
print(report)

# Criar matrizes de confusão para cada classe
cm_matrices = multilabel_confusion_matrix(Y_test, Y_pred)

# Plotar as matrizes de confusão separadas por classe
for i, label in enumerate(labels):
    cm = cm_matrices[i].astype('float') / cm_matrices[i].sum(axis=1, keepdims=True) * 100  # Normalizar para porcentagem

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues",
                xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
    plt.title(f"Matriz de Confusão - {label} (%)")
    plt.xlabel("Predito")
    plt.ylabel("Real")
    plt.show()

"""**ANALYSIS QUESTION** What do you observe? What can you conclude?

Foi melhorado grandemente em comparação com os dados sem balanceamento, uma vez que retirei uma parte das amostras que não possuiam falhas, pois o numero era grande demais comparado com maquinas que tinham falhas. Outro ponto a ser analisado é que meu modelo atual ainda não possui perfeita precisão, pois ele aponta muitos falsos positivos, porém não vejo como algo crítico, pois o caso de não previsão quando há falhas pode acarretar em perdas maiores.
>>>>>>> Stashed changes
"""