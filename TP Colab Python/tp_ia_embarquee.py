# -*- coding: utf-8 -*-
"""TP_IA_EMBARQUEE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iKHeXPCFbghnl78NwaLu2nLqccQaNjBJ

## **PRACTICAL SESSION 1** — Deep Learning for predictive maintenance

The dataset used is the **AI4I 2020** Predictive Maintenance Dataset, which contains 10,000 instances of industrial sensor data. Each instance represents the operating condition of a machine and is associated with a label indicating whether a failure has occurred and, if so, what type of failure it is.

The 5 possible labels are:



*   **TWF**: Tool Wear Failure
*   **HDF**: Heat Dissipation Failure
*   **PWF**: Power Failure
*   **OSF**: Overstrain Failure
*   **RNF**: Random Failure


The data is available on eCAMPUS as CSV file called: "ai4i2020.csv"

## **PRACTICAL SESSION Goal** — Ceate a deep leanring model allowing to realize a predictive maintenance mission

## **1 - Analysis of the dataset**

All libraries used ***SHOULD BE PLACED*** in the code cell below
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#TensorFlow e Keras
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2
from tensorflow.keras.metrics import AUC
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

#Scikit-Learn
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.metrics import (
    confusion_matrix, classification_report, roc_curve, auc,
    ConfusionMatrixDisplay, multilabel_confusion_matrix
)
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

#Imbalanced-Learn
from imblearn.over_sampling import SMOTE, SMOTENC
from imblearn.under_sampling import RandomUnderSampler

"""**QUESTION:** Load dataset and display some lines of the csv file."""

data_path = "/content/ai4i2020.csv"


df = pd.read_csv(data_path)


df.head()

"""**QUESTION:** Display the distribution of machine failures and non-failures with a bar graph."""

counts_machine_failure = df['Machine failure'].value_counts()


labels = ['No Failure', 'Failure']
values = [counts_machine_failure.get(0, 0), counts_machine_failure.get(1, 0)]


plt.figure(figsize=(6, 4))
plt.bar(labels, values, color=['blue', 'red'])
plt.title("Distribution of Machine Failures vs Non-Failures")
plt.xlabel("Failure Status")
plt.ylabel("Number of Machines")


for i, v in enumerate(values):
    plt.text(i, v + 0.5, str(int(v)), ha='center')

plt.show()

"""**ANALYSIS QUESTION:** What do you observe?

Há muitas maquinas que não houveram falhas em compararação com máquinas que tiveram falhas.

**ANALYSIS QUESTION:** What will be the consequence of this phenomenon on the model's learning?

Porteriormente ao usar esses dados para treinar a IA, pode acarretar em um bom aprendizado para detectar apenas quando não há falhas, resultando em mal aprendizado para identificar quando há falhas também.

**QUESTION:** Create a bar chart showing the distribution of different failure types (TWF, HDF, PWF, OSF, RNF). Display the exact values above each bar in the chart."
"""

failure_types = ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']
failure_counts = df[failure_types].sum()

plt.figure(figsize=(8, 6))
ax = sns.barplot(x=failure_counts.index, y=failure_counts.values, palette=['blue', 'orange', 'green', 'red', 'purple'] )

for i, value in enumerate(failure_counts.values):
    ax.text(i, value + 2, str(value), ha='center', fontsize=12)

plt.xlabel("Type of fault")
plt.ylabel("Number of Occurrences")
plt.title("Distribution of Fault Types")

plt.show()

"""**ANALYSIS QUESTION:** What do you observe?

Mesmo em comparação com as maquinas que tiveram falhas, ainda sim há um diferença entre os tipos de falhas, onde 19 foram categorizadas como RNF e 115 foram caracterizadas como HDF.

**QUESTION:** Create a bar chart showing the distribution of failure types (TWF, HDF, PWF, OSF, RNF) among machines that experienced a failure (Machine failure == 1). Additionally, add a "No Specific Failure" category to count cases where a machine failed but no specific failure type was recorded. Display the exact values above each bar in the chart."
"""

failed_machines = df[df["Machine failure"] == 1]

failure_types = ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']
failure_counts = failed_machines[failure_types].sum()

no_specific_failure = df[(df["Machine failure"] == 0) & (df["RNF"] == 1)].shape[0]

failure_counts["No Specific Failure"] = no_specific_failure

plt.figure(figsize=(8, 6))
ax = sns.barplot(x=failure_counts.index, y=failure_counts.values,  palette=['blue', 'orange', 'green', 'red', 'purple', 'gray'])

for i, value in enumerate(failure_counts.values):
    ax.text(i, value + 2, str(value), ha='center', fontsize=12)

plt.xlabel("Type of fault")
plt.ylabel("Number of Occurrences")
plt.title("Distribution of Fault Types among Faulty Machines")

plt.show()

"""**ANALYSIS QUESTION:** What do you obsrve comapred to the previous question ? What can you conclude?

Mais uma vez há uma discrepancia no número de amostras nas maquinás que obtiveram falhas, onde apenas 1 foi caracterizada como RNF e 115 como HDF

**QUESTION:** Display the names of the different columns in the dataset with their respective data types.
"""

print("Column Names and Data Types:")
print(df.dtypes)

"""**ANALYSIS QUESTION:** To train the model, what will be the inputs and outputs (What are the names of the columns that you will use?)? Justify your response.
Remember, you want to predict if the machine will fail, and if so, what kind of failure. You need to yse previous results to jsurtify your response.

Para treinar o modelo, as entradas escolhidas serão as variáveis que indicam o uso da máquina e situação fisica dela, como Air temperature, Process temperature, Rotational speed, Torque, Tool wear e Type, pois influenciam diretamente no desempenho e possíveis falhas. As saídas
serão Machine failure e as colunas com os tipos de falhas como TWF, HDF, PWF e OSF, retirei RNF pois sua quantidade é irrisória em comparação com as outras falhas e principalmente porque são falhas aleatórias. Isso acarreta em uma previsão sobre os possiveis problemas além de permitir ações de manutenção preditiva baseadas nas condições da máquina.

## **2- Train model Without balancing the dataset**

---

In this section, you must build and train a model without rebalancing the dataset.

**QUESTION:** Create X_train, Y_train, X_test, and Y_test. How many elements are present in X_train, Y_train, X_test, and Y_test? (Print the values)
"""

# Criar variáveis dummies para a coluna 'Type'
df = pd.get_dummies(df, columns=['Type'], drop_first=False)

# Definir X (remover IDs e variáveis de saída)
X = df.drop(columns=['UDI', 'Product ID', 'Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'])

# Definir y (agora multi-label: cada falha é independente)
Y = df[['Machine failure','TWF', 'HDF', 'PWF', 'OSF']]

# Criar coluna "No Failure"
#Y['No Failure'] = (df['Machine failure'] == 0).astype(int)

# Normalizar X para intervalo [0,1]
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

# print das colunas de x e y
print(X.head())
print(Y.head())
# Divisão treino (70%) e teste (30%)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Exibir dimensões
print(f"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}")
print(f"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}")

"""



**QUESTION** Code below the model architecture"""

# Criar o modelo aprimorado para multi-label
model = Sequential([
    Dense(64, kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),
    BatchNormalization(),
    LeakyReLU(),
    Dropout(0.3),

    Dense(32, kernel_regularizer=l2(0.005)),
    BatchNormalization(),
    LeakyReLU(),
    Dropout(0.2),

    Dense(5, activation='sigmoid')  # 5 classes: Machine Failure, TWF, HDF, PWF, OSF
])

# Compilar o modelo com métricas ajustadas
model.compile(optimizer=AdamW(learning_rate=0.001, weight_decay=0.02),
              loss='binary_crossentropy',  # Multi-label classification
              metrics=['binary_accuracy', AUC(name='auc')])

# Exibir resumo
model.summary()

"""**QUESTION** Code below the algorithms allowing to train model

**WARNING!** You need to plot the training and test accuracy and loss to check if our model is overfitting
"""

# Definir callbacks otimizados
early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)

# Treinar modelo
history = model.fit(
    X_train, Y_train,
    validation_data=(X_test, Y_test),
    epochs=50,
    batch_size=64,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

#Função para plotar métricas corretamente para multi-label
def plot_metric(history, metric, title, ylabel, loc='upper left'):
    if metric in history.history:
        plt.plot(history.history[metric], label='Treino')
    if f'val_{metric}' in history.history:
        plt.plot(history.history[f'val_{metric}'], label='Teste')

    plt.title(title)
    plt.xlabel('Época')
    plt.ylabel(ylabel)
    plt.legend(loc=loc)
    plt.show()

#Plotar a perda (loss)
plot_metric(history, 'loss', 'Perda (Loss) do Modelo', 'Loss', loc='upper right')

#  Plotar acurácia binária (para multi-label)
plot_metric(history, 'binary_accuracy', 'Acurácia Binária do Modelo', 'Acurácia')

#  Plotar AUC (Área Sob a Curva ROC)
plot_metric(history, 'auc', 'Área Sob a Curva ROC (AUC)', 'AUC')

"""**QUESTION** Plot the confusion matrix and the classification report

**Tips:**

*   classification report link

> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html

*   Matrix confusion

> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html
"""

# Fazer previsões em probabilidades
Y_pred_probs = model.predict(X_test)

# Ajustar threshold dinamicamente baseado na média das probabilidades
thresholds = np.mean(Y_pred_probs, axis=0)  # Calcula média por classe
Y_pred = (Y_pred_probs >= thresholds).astype(int)  # Ajusta decisão para cada classe

#  Gerar relatório de classificação
labels = ['Machine Failure', 'TWF', 'HDF', 'PWF', 'OSF']
report = classification_report(Y_test, Y_pred, target_names=labels)
print("\nRelatório de Classificação:")
print(report)

# Criar matrizes de confusão para cada classe
cm_matrices = multilabel_confusion_matrix(Y_test, Y_pred)

# Plotar as matrizes de confusão separadas por classe
for i, label in enumerate(labels):
    cm = cm_matrices[i].astype('float') / cm_matrices[i].sum(axis=1, keepdims=True) * 100  # Normalizar para porcentagem

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues",
                xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
    plt.title(f"Matriz de Confusão - {label} (%)")
    plt.xlabel("Predito")
    plt.ylabel("Real")
    plt.show()

"""**ANALYSIS QUESTION** What do you observe? What can you conclude?

O modelo ficou bom em prever quando não há falhas, porém ele não consegue prever quando há, e sempre a respota que prevalece é que não houve, dado que o numero de maquinas sem falhas é muito maior em comparação com as maquinas com falhas.

## **3- Train model With balancing the dataset**

---

Methods for rebalancing a dataset:


*   Use oversampling techniques (e.g., SMOTE) to generate synthetic data for minority classes


> https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html



*   Apply undersampling techniques (e.g., random undersampling, Tomek Links, Edited Nearest Neighbors) to reduce the majority class size



> https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html



*   Use class weighting during model training to penalize errors on minority classes



> https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?hl=fr

**QUESTION:** Create X_train, Y_train, X_test, and Y_test. How many elements are present in X_train, Y_train, X_test, and Y_test? (Print the values)
"""

data_path = "/content/ai4i2020.csv"
df = pd.read_csv(data_path)

#separa os tipos das maquinas em booleanos
df = pd.get_dummies(df, columns=['Type'], drop_first=False)

#Definir X e Y antes do balanceamento
X = df.drop(columns=['UDI', 'Product ID', 'Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'])
Y = df[['Machine failure', 'TWF', 'HDF', 'PWF', 'OSF']]

# Aplicar Undersampling APENAS para `Machine Failure`
undersampler = RandomUnderSampler(sampling_strategy=0.3, random_state=42)  # Mantém 30% dos "No Failure"
X_resampled, y_machine_failure = undersampler.fit_resample(X, Y['Machine failure'])  # Apenas Machine Failure

# Recuperar os índices selecionados pelo undersampling
selected_indices = y_machine_failure.index

# Filtrar `Y` completo para manter os rótulos corretos das amostras balanceadas
Y_resampled = Y.loc[selected_indices]

# Normalizar X para intervalo [0,1]
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_resampled)
X_resampled = pd.DataFrame(X_scaled, columns=X.columns)

# Agora dividir em treino (70%) e teste (30%)
X_train, X_test, Y_train, Y_test = train_test_split(
    X_resampled, Y_resampled, test_size=0.2, random_state=42)

# Exibir dimensões finais
print(f"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}")
print(f"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}")

X_train, Y_train
X_test, Y_test

# Contar a quantidade de máquinas com e sem falha após o undersampling
failure_counts = Y_resampled['Machine failure'].value_counts()

# Plotar a distribuição das máquinas após o undersampling
plt.figure(figsize=(6, 4))
ax = sns.barplot(x=failure_counts.index, y=failure_counts.values, palette=['blue', 'red'])

# Adicionar valores acima das barras
for i, value in enumerate(failure_counts.values):
    ax.text(i, value + 2, str(value), ha='center', fontsize=12)

plt.xlabel("Machine Failure (0 = Sem Falha, 1 = Com Falha)")
plt.ylabel("Número de Máquinas")
plt.title("Distribuição de Máquinas com e sem Falha após Undersampling")

# Exibir o gráfico
plt.show()

"""**ANALYSIS QUESTION:** Explain the choices you made to balance the dataset.

Eu fiz a mistura de SMOTE e class weigths, pois com o SMOTE, consigo criar novas amostras das Maquinas com Falhas, enquanto que o classh weigths me permite modificar os pesos das variaveis de treino, assim consigo dar mais peso para as variaveis em menor numero e menor peso para as variaveis abundantes.

**QUESTION:** Code below the model architecture

**TIP:** It could be interesting to keep it the same as before
"""

# Criar o modelo aprimorado para multi-label
model = Sequential([
    Dense(64, kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),
    BatchNormalization(),
    LeakyReLU(),
    Dropout(0.3),

    Dense(32, kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    LeakyReLU(),
    Dropout(0.2),

    Dense(5, activation='sigmoid')  # 5 classes: Machine Failure, TWF, HDF, PWF, OSF
])

# Compilar o modelo com métricas ajustadas
model.compile(optimizer=AdamW(learning_rate=0.001, weight_decay=0.01),
              loss='binary_crossentropy',  # Multi-label classification
              metrics=['binary_accuracy', AUC(name='auc')])

# Exibir resumo
model.summary()

"""**QUESTION** Code below the algorithms allowing to train model

"""

# Definir callbacks otimizados
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)

# Criar dicionário de class_weight no formato correto para o modelo Keras
class_weight_dict = {i: class_weights[i][1] for i in range(len(class_weights))}

# Treinar o modelo com os dados balanceados e pesos ajustados
history = model.fit(
    X_train,
    Y_train,
    validation_data=(X_test, Y_test),
    epochs=50,
    batch_size=64,
    #class_weight=class_weight_dict,  # Aplicando class weights automáticos
    #class_weight=custom_class_weights,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)




#Função para plotar métricas corretamente para multi-label
def plot_metric(history, metric, title, ylabel, loc='upper left'):
    if metric in history.history:
        plt.plot(history.history[metric], label='Treino')
    if f'val_{metric}' in history.history:
        plt.plot(history.history[f'val_{metric}'], label='Teste')

    plt.title(title)
    plt.xlabel('Época')
    plt.ylabel(ylabel)
    plt.legend(loc=loc)
    plt.show()

#Plotar a perda (loss)
plot_metric(history, 'loss', 'Perda (Loss) do Modelo', 'Loss', loc='upper right')

#  Plotar acurácia binária (para multi-label)
plot_metric(history, 'binary_accuracy', 'Acurácia Binária do Modelo', 'Acurácia')

#  Plotar AUC (Área Sob a Curva ROC)
plot_metric(history, 'auc', 'Área Sob a Curva ROC (AUC)', 'AUC')

"""**QUESTION** Plot the confusion matrix and the classification report"""

# Fazer previsões em probabilidades
Y_pred_probs = model.predict(X_test)

# Ajustar threshold dinamicamente baseado na média das probabilidades
thresholds = np.mean(Y_pred_probs, axis=0)  # Calcula média por classe
Y_pred = (Y_pred_probs >= thresholds).astype(int)  # Ajusta decisão para cada classe

#  Gerar relatório de classificação
labels = ['Machine Failure', 'TWF', 'HDF', 'PWF', 'OSF']
report = classification_report(Y_test, Y_pred, target_names=labels)
print("\nRelatório de Classificação:")
print(report)

# Criar matrizes de confusão para cada classe
cm_matrices = multilabel_confusion_matrix(Y_test, Y_pred)

# Plotar as matrizes de confusão separadas por classe
for i, label in enumerate(labels):
    cm = cm_matrices[i].astype('float') / cm_matrices[i].sum(axis=1, keepdims=True) * 100  # Normalizar para porcentagem

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues",
                xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
    plt.title(f"Matriz de Confusão - {label} (%)")
    plt.xlabel("Predito")
    plt.ylabel("Real")
    plt.show()

"""**ANALYSIS QUESTION** What do you observe? What can you conclude?

Foi melhorado grandemente em comparação com os dados sem balanceamento, uma vez que retirei uma parte das amostras que não possuiam falhas, pois o numero era grande demais comparado com maquinas que tinham falhas. Outro ponto a ser analisado é que meu modelo atual ainda não possui perfeita precisão, pois ele aponta muitos falsos positivos, porém não vejo como algo crítico, pois o caso de não previsão quando há falhas pode acarretar em perdas maiores.
"""